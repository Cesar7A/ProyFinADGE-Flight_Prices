# ======================================
# 4) script spark_tron_pyspark.py
# ======================================
#!/usr/bin/env python3


import sys, math, argparse
import numpy as np
from pyspark.sql import SparkSession
from pyspark.mllib.util import MLUtils

# ----------------------------
# Utilities: LIBSVM -> AA (idxs, vals, y)
# ----------------------------
def libsvm_to_AA_rdd(sc, path, repartitions=None):
    """
    Carga LIBSVM con MLUtils, devuelve RDD de (idxs_array, vals_array, label)
    además de numFeatures
    """
    data = MLUtils.loadLibSVMFile(sc, path)  # LabeledPoint with sparse vector
    rdd = data.map(lambda lp: (lp.label, lp.features))  # (label, SparseVector)
    # convertir a (idxs, vals, label)
    def to_aa(x):
        y, sv = x
        # sv.indices and sv.values for mllib.linalg.SparseVector
        # but to be safe, use toArray() only for dimension detection later
        # We'll return python lists for indices/vals to keep serialization simple
        if hasattr(sv, "indices"):  # SparseVector
            idxs = sv.indices.tolist()
            vals = sv.values.tolist()
        else:
            # fallback: scan array
            arr = sv.toArray()
            idxs = [i for i, v in enumerate(arr) if v != 0.0]
            vals = [arr[i] for i in idxs]
        return (idxs, vals, float(y))
    aa_rdd = rdd.map(to_aa)
    if repartitions:
        aa_rdd = aa_rdd.repartition(repartitions)
    aa_rdd = aa_rdd.cache()
    # detect numFeatures
    max_idx = aa_rdd.map(lambda t: max(t[0]) if len(t[0])>0 else -1).reduce(lambda a,b: a if a>b else b)
    numFeatures = max_idx + 1 if max_idx >= 0 else 0
    return aa_rdd, numFeatures

# ----------------------------
# Partition-level computations (mapPartitions)
# ----------------------------
def partition_grad_loss(iterator, w_local, model, C):
    """
    iterator yields (idxs, vals, y)
    returns (grad_dense, loss_sum)
    """
    if w_local is None:
        return
    n = len(w_local)
    grad = np.zeros(n, dtype=float)
    loss = 0.0
    for idxs, vals, y in iterator:
        # sparse dot
        xw = 0.0
        for i, v in zip(idxs, vals):
            xw += v * w_local[i]
        if model == "LR":
            yz = y * xw
            # stable logistic loss
            if yz > 0:
                loss_i = math.log1p(math.exp(-yz))
            else:
                loss_i = -yz + math.log1p(math.exp(yz))
            loss += loss_i
            expTerm = math.exp(-yz)
            coeff = -y * (expTerm / (1.0 + expTerm))
            if coeff != 0.0:
                for i, v in zip(idxs, vals):
                    grad[i] += v * coeff
        else: # L2-SVM squared hinge
            margin = y * xw
            if margin < 1.0:
                # loss = 0.5*(1-margin)^2
                loss += 0.5 * (1.0 - margin)**2
                coeff = -(1.0 - margin) * y
                for i, v in zip(idxs, vals):
                    grad[i] += v * coeff
            else:
                # no contribution
                pass
    yield (grad, loss)

def partition_hv(iterator, w_local, v_local, model, C):
    """
    Compute partition contribution to sum_i D_i * (x_i^T v) * x_i
    For LR: D_i = exp(-y xw)/(1+exp(-y xw))^2
    For L2-SVM: generalized Hessian uses indicator; for squared hinge,
      D_i = 1 if margin < 1 else 0 (and we use x_i*x_i^T scaled by (x_i^T v))
      For squared hinge the second derivative is 1 for margin<1 (since loss 0.5(1 - yxw)^2 -> Hessian = y^2 x x^T = x x^T)
    Returns dense vector (numpy)
    """
    if w_local is None or v_local is None:
        return
    n = len(w_local)
    out = np.zeros(n, dtype=float)
    for idxs, vals, y in iterator:
        xw = 0.0
        for i, val in zip(idxs, vals):
            xw += val * w_local[i]
        xv = 0.0
        for i, val in zip(idxs, vals):
            xv += val * v_local[i]
        if model == "LR":
            ex = math.exp(-y * xw)
            denom = (1.0 + ex)
            Di = ex / (denom * denom)
            coeff = Di * xv
            if coeff != 0.0:
                for i, val in zip(idxs, vals):
                    out[i] += val * coeff
        else: # L2-SVM squared hinge
            margin = y * xw
            if margin < 1.0:
                # D_i = 1   (since second derivative of 0.5*(1 - margin)^2 w.r.t w is (x x^T))
                coeff = xv
                if coeff != 0.0:
                    for i, val in zip(idxs, vals):
                        out[i] += val * coeff
            else:
                pass
    yield out

# ----------------------------
# Helpers to aggregate partitions
# ----------------------------
def aggregate_partitions(rdd, mapfunc, zero_vec):
    """
    Generic aggregate using mapPartitions(mapfunc) where mapfunc yields a dense numpy array or (dense, scalar)
    zero_vec: initial for reduce
    """
    parts = rdd.mapPartitions(mapfunc)
    # reduce carefully: parts elements might be numpy arrays or tuples
    first = parts.take(1)
    if len(first) == 0:
        return zero_vec
    # We'll use aggregate with python ops to avoid high-memory collect
    def seq_op(a, b):
        # a and b are same type: numpy array or tuple (arr, scalar)
        if isinstance(a, tuple):
            arr_a, s_a = a
            arr_b, s_b = b
            arr_a += arr_b
            return (arr_a, s_a + s_b)
        else:
            a += b
            return a
    def comb_op(a, b):
        return seq_op(a, b)
    return parts.aggregate(zero_vec, seq_op, comb_op)

# ----------------------------
# Compute objective f(w) and gradient g(w)
# ----------------------------
def compute_obj_and_grad(rdd, sc, w, model, C, coalesceTo=None):
    """
    Returns (f, g) where g is numpy dense
    Uses mapPartitions to compute partition grad and loss, then reduce
    """
    n = len(w)
    w_b = sc.broadcast(w)
    # mapPartitions function uses local broadcast value
    def mp(iter_):
        return partition_grad_loss(iter_, w_b.value, model, C)
    if coalesceTo:
        rdd2 = rdd.coalesce(coalesceTo)
    else:
        rdd2 = rdd
    # initialize zero
    zero = (np.zeros(n, dtype=float), 0.0)
    parts = rdd2.mapPartitions(mp)
    # aggregate
    agg = parts.aggregate(zero,
                          lambda a, b: (a[0] + b[0], a[1] + b[1]),
                          lambda a, b: (a[0] + b[0], a[1] + b[1]))
    grad_part = agg[0]
    loss_sum = agg[1]
    # gradient final
    g = grad_part * C + w  # note: objective is 0.5||w||^2 + C * sum loss_i
    f = 0.5 * float(np.dot(w, w)) + C * loss_sum
    w_b.unpersist()
    return f, g

# ----------------------------
# Hessian-vector product Hv operator (uses mapPartitions)
# ----------------------------
def hessian_vector_product(rdd, sc, w, v, model, C, coalesceTo=None):
    """
    Hv = v + C * sum_k ( partition_Hv_k )
    partition_Hv_k computed by mapPartitions
    """
    n = len(w)
    w_b = sc.broadcast(w)
    v_b = sc.broadcast(v)
    def mp(iter_):
        return partition_hv(iter_, w_b.value, v_b.value, model, C)
    rdd2 = rdd.coalesce(coalesceTo) if coalesceTo else rdd
    # initialize zero vector
    zero = np.zeros(n, dtype=float)
    parts = rdd2.mapPartitions(mp)
    agg = parts.aggregate(zero,
                          lambda a, b: a + b,
                          lambda a, b: a + b)
    Hv = v + C * agg
    w_b.unpersist()
    v_b.unpersist()
    return Hv

# ----------------------------
# CG solver with trust-region boundary handling
# ----------------------------
def conjugate_gradient_trust(hv_func, g, delta, cgMaxIter=50, xi=0.1, tol_rel=1e-4):
    """
    Solve approximately min 1/2 d^T H d + g^T d  s.t. ||d|| <= delta
    using CG with early stopping and trust-region boundary.
    hv_func: function(v) -> H v
    g: gradient (dense numpy) (we solve H d = -g)
    xi: stopping parameter xi_t (||r|| <= xi * ||g||)
    Returns d (step), flag (0=converged inner, 1=hit boundary, 2=iter limit)
    """
    n = len(g)
    dbar = np.zeros(n, dtype=float)   # dbar_i in algorithm
    r = -g.copy()                     # r0 = -g
    s = r.copy()                      # s0 = r0
    rs_old = float(np.dot(r, r))
    gnorm = math.sqrt(float(np.dot(g, g)))
    rel_tol = (xi * gnorm)**2
    if rs_old <= rel_tol:
        return dbar, 0  # already satisfies inner tolerance
    for i in range(cgMaxIter):
        # compute u = H s
        u = hv_func(s)
        pTs_u = float(np.dot(s, u))
        if pTs_u <= 0:
            # Nonpositive curvature -> return the point on boundary in direction s
            # find tau s.t. ||dbar + tau s|| = delta
            s_norm2 = float(np.dot(s, s))
            if s_norm2 == 0:
                return dbar, 2
            # solve quadratic for tau: ||dbar + tau s||^2 = delta^2
            # (s^T s) tau^2 + 2 (dbar^T s) tau + (dbar^T dbar - delta^2) = 0
            a = s_norm2
            b = 2.0 * float(np.dot(dbar, s))
            c = float(np.dot(dbar, dbar) - delta*delta)
            disc = b*b - 4*a*c
            if disc < 0:
                tau = 0.0
            else:
                tau = (-b + math.sqrt(disc)) / (2*a)
                if tau < 0:
                    tau = (-b - math.sqrt(disc)) / (2*a)
            d = dbar + tau * s
            return d, 1
        alpha = rs_old / pTs_u
        dbar_new = dbar + alpha * s
        # check trust-region boundary
        if np.linalg.norm(dbar_new) >= delta:
            # find tau in (0, alpha] s.t. ||dbar + tau*s|| = delta
            # solve quadratic (for tau)
            a = float(np.dot(s, s))
            b = 2.0 * float(np.dot(dbar, s))
            c = float(np.dot(dbar, dbar) - delta*delta)
            disc = b*b - 4*a*c
            if disc < 0:
                tau = alpha
            else:
                sqrt_disc = math.sqrt(disc)
                tau1 = (-b + sqrt_disc) / (2*a)
                tau2 = (-b - sqrt_disc) / (2*a)
                # choose tau in [0, alpha]
                tau_candidates = [t for t in [tau1, tau2] if 0.0 <= t <= alpha + 1e-12]
                if len(tau_candidates) == 0:
                    tau = min(max(tau1, 0.0), alpha)
                else:
                    tau = max(tau_candidates)
            d = dbar + tau * s
            return d, 1
        # normal CG update
        dbar = dbar_new
        r = r - alpha * u
        rs_new = float(np.dot(r, r))
        if rs_new <= rel_tol:
            return dbar, 0
        beta = rs_new / rs_old
        s = r + beta * s
        rs_old = rs_new
    return dbar, 2

# ----------------------------
# TRON full algorithm
# ----------------------------
def tron_full(rdd, sc, numFeatures, model="LR", C=1.0,
              maxIter=20, cgMaxIter=50, xi=0.1,
              delta0=1.0, deltaMax=1e6, eta=0.01,
              coalesceTo=None):
    """
    rdd: RDD of (idxs, vals, y) cached
    Returns w
    """
    n = int(numFeatures)
    w = np.zeros(n, dtype=float)
    delta = float(delta0)
    for t in range(maxIter):
        # 1) compute f and grad
        f_w, g = compute_obj_and_grad(rdd, sc, w, model, C, coalesceTo)
        gradNorm = float(np.linalg.norm(g))
        print(f"[TRON] iter={t} f={f_w:.6g} ||g||={gradNorm:.6g} delta={delta:.6g}")

        if gradNorm < 1e-8:
            print("[TRON] criterio de paro (gradNorm)")
            break

        # 2) define hv operator using current w
        def hv_op(v):
            return hessian_vector_product(rdd, sc, w, v, model, C, coalesceTo)

        # 3) run CG with trust-region
        d, flag = conjugate_gradient_trust(hv_op, g, delta, cgMaxIter=cgMaxIter, xi=xi)
        # compute q(d) = g^T d + 0.5 d^T H d
        Hd = hv_op(d)
        qd = float(np.dot(g, d) + 0.5 * np.dot(d, Hd))

        # 4) compute f(w + d) (actual)
        w_plus = w + d
        f_wpd, _ = compute_obj_and_grad(rdd, sc, w_plus, model, C, coalesceTo)
        # Note compute_obj_and_grad returns gradient too, but we only need f
        # 5) compute rho_t
        # as paper: rho = (f(w+ d) - f(w)) / q(d)
        # careful: qd should be negative (we expect descent), but we follow their def
        numer = f_wpd - f_w
        denom = qd
        rho = numer / denom if denom != 0.0 else float('inf')

        print(f"  flag={flag} qd={qd:.6g} f(w+d)={f_wpd:.6g} rho={rho:.6g}")

        # 6) decide acceptance
        if rho > eta:
            # accept
            w = w_plus
            # update delta
            if rho > 0.75 and np.linalg.norm(d) >= 0.99 * delta:
                delta = min(2.0 * delta, deltaMax)
        else:
            # reject: keep w
            # reduce delta
            delta = 0.25 * delta

        # additional stopping if small step
        if np.linalg.norm(d) < 1e-12:
            print("[TRON] paso muy pequeño, saliendo.")
            break
    return w

# ----------------------------
# Main: parse args and run
# ----------------------------
def parse_args():
    p = argparse.ArgumentParser()
    p.add_argument("data", type=str, help="ruta a archivo LIBSVM (hdfs:// o file://)")
    p.add_argument("--model", choices=["LR", "L2SVM"], default="LR")
    p.add_argument("--C", type=float, default=1.0)
    p.add_argument("--maxIter", type=int, default=20)
    p.add_argument("--cgMaxIter", type=int, default=50)
    p.add_argument("--xi", type=float, default=0.1)
    p.add_argument("--delta0", type=float, default=1.0)
    p.add_argument("--deltaMax", type=float, default=1e6)
    p.add_argument("--eta", type=float, default=0.01)
    p.add_argument("--coalesceTo", type=int, default=0)
    p.add_argument("--repart", type=int, default=0)
    return p.parse_args()

if __name__ == "__main__":
    args = parse_args()
    spark = SparkSession.builder.appName("PySparkTRONFull").getOrCreate()
    sc = spark.sparkContext
    print("== Cargando datos (LIBSVM) ==")
    repart = args.repart if args.repart > 0 else None
    rdd, numFeatures = libsvm_to_AA_rdd(sc, args.data, repartitions=repart)
    if args.coalesceTo and args.coalesceTo > 0:
        coalesceTo = args.coalesceTo
    else:
        coalesceTo = None
    print(f"> numFeatures = {numFeatures}, partitions = {rdd.getNumPartitions()}")
    if numFeatures == 0:
        print("ERROR: numFeatures=0. Revisa dataset.")
        sys.exit(1)
    print("== Empezando TRON completo (con optimizaciones) ==")
    w = tron_full(rdd, sc, numFeatures, model=args.model, C=args.C,
                  maxIter=args.maxIter, cgMaxIter=args.cgMaxIter, xi=args.xi,
                  delta0=args.delta0, deltaMax=args.deltaMax, eta=args.eta,
                  coalesceTo=coalesceTo)
    print("== Resultado (primeros 100 pesos) ==")
    for i, val in enumerate(w[:100]):
        print(f"{i}\t{val:.6g}")
    spark.stop()
